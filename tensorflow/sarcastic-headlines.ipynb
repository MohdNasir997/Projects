{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-29 16:13:02.467560: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-29 16:13:09.971911: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-29 16:13:09.971952: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-29 16:13:25.321172: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-29 16:13:25.321440: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-29 16:13:25.321464: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import json\n",
    "import numpy as np\n",
    "import os  \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>headline</th>\n",
       "      <th>article_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_sarcastic                                           headline  \\\n",
       "0             1  thirtysomething scientists unveil doomsday clo...   \n",
       "1             0  dem rep. totally nails why congress is falling...   \n",
       "2             0  eat your veggies: 9 deliciously different recipes   \n",
       "3             1  inclement weather prevents liar from getting t...   \n",
       "4             1  mother comes pretty close to using word 'strea...   \n",
       "\n",
       "                                        article_link  \n",
       "0  https://www.theonion.com/thirtysomething-scien...  \n",
       "1  https://www.huffingtonpost.com/entry/donna-edw...  \n",
       "2  https://www.huffingtonpost.com/entry/eat-your-...  \n",
       "3  https://local.theonion.com/inclement-weather-p...  \n",
       "4  https://www.theonion.com/mother-comes-pretty-c...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']= '-1'\n",
    "sentences = []\n",
    "labels = []\n",
    "urls = []    \n",
    "datastore = pd.read_json('/home/nasir/Desktop/python practice/data/archive(1)/Sarcasm_Headlines_Dataset_v2.json',lines=True)\n",
    "datastore.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = datastore['headline']\n",
    "urls = datastore['article_link']\n",
    "labels = datastore['is_sarcastic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenzer = keras.preprocessing.text.Tokenizer(oov_token='<OOV>')\n",
    "tokenzer.fit_on_texts(sentences)\n",
    "word_index = tokenzer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16004   355  3167  7474  2644     3   661  1119     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "(28619, 152)\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenzer.texts_to_sequences(sentences)\n",
    "padding = keras.preprocessing.sequence.pad_sequences(sequences,padding='post')\n",
    "print(padding[0])\n",
    "print(padding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 20000\n",
    "training_sentences = sentences[0:training_size]\n",
    "testing_sentences = sentences[training_size:]\n",
    "training_label = labels[0:training_size]\n",
    "testing_label = labels[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "max_lenght = 100\n",
    "embedding_dim = 16\n",
    "tokenzer =  keras.preprocessing.text.Tokenizer(num_words= vocab_size,oov_token='<OOV>' )\n",
    "tokenzer.fit_on_texts(training_sentences)\n",
    "word_index = tokenzer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenzer.texts_to_sequences(training_sentences)\n",
    "training_padding = keras.preprocessing.sequence.pad_sequences(sequences,maxlen=max_lenght,padding='post',truncating='post')\n",
    "testing_sequences = tokenzer.texts_to_sequences(testing_sentences)\n",
    "testing_padding =  keras.preprocessing.sequence.pad_sequences(testing_sequences,maxlen=max_lenght,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([keras.layers.Embedding(vocab_size,embedding_dim,input_length = max_lenght),keras.layers.GlobalAveragePooling1D(),keras.layers.Dense(24,activation = 'relu'),keras.layers.Dense(1,activation = 'sigmoid')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1130 - acc: 0.9610 - val_loss: 0.4406 - val_acc: 0.8386\n",
      "Epoch 2/20\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1037 - acc: 0.9652 - val_loss: 0.4675 - val_acc: 0.8344\n",
      "Epoch 3/20\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.0936 - acc: 0.9689 - val_loss: 0.4978 - val_acc: 0.8320\n",
      "Epoch 4/20\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.0859 - acc: 0.9719 - val_loss: 0.5272 - val_acc: 0.8296\n",
      "Epoch 5/20\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.0789 - acc: 0.9740 - val_loss: 0.5805 - val_acc: 0.8254\n",
      "Epoch 6/20\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0735 - acc: 0.9758 - val_loss: 0.5908 - val_acc: 0.8249\n",
      "Epoch 7/20\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0674 - acc: 0.9781 - val_loss: 0.6272 - val_acc: 0.8238\n",
      "Epoch 8/20\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0627 - acc: 0.9800 - val_loss: 0.6577 - val_acc: 0.8245\n",
      "Epoch 9/20\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.0557 - acc: 0.9829 - val_loss: 0.6910 - val_acc: 0.8225\n",
      "Epoch 10/20\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.0506 - acc: 0.9849 - val_loss: 0.7288 - val_acc: 0.8177\n",
      "Epoch 11/20\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.0484 - acc: 0.9857 - val_loss: 0.7628 - val_acc: 0.8165\n",
      "Epoch 12/20\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.0427 - acc: 0.9881 - val_loss: 0.8340 - val_acc: 0.8119\n",
      "Epoch 13/20\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.0403 - acc: 0.9884 - val_loss: 0.8472 - val_acc: 0.8109\n",
      "Epoch 14/20\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.0377 - acc: 0.9883 - val_loss: 0.8723 - val_acc: 0.8134\n",
      "Epoch 15/20\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.0362 - acc: 0.9894 - val_loss: 0.9075 - val_acc: 0.8120\n",
      "Epoch 16/20\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.0330 - acc: 0.9900 - val_loss: 0.9484 - val_acc: 0.8101\n",
      "Epoch 17/20\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.0291 - acc: 0.9923 - val_loss: 1.0075 - val_acc: 0.8087\n",
      "Epoch 18/20\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.0282 - acc: 0.9919 - val_loss: 1.0325 - val_acc: 0.8057\n",
      "Epoch 19/20\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.0247 - acc: 0.9931 - val_loss: 1.0753 - val_acc: 0.8038\n",
      "Epoch 20/20\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.0234 - acc: 0.9934 - val_loss: 1.1123 - val_acc: 0.8047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdedc07e4c0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(training_padding,training_label,epochs=20,validation_data=(testing_padding,testing_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ['This thing does not work','you think, you are smart','holy cow, you think you smart']\n",
    "new_sequence = tokenzer.texts_to_sequences(sentence)\n",
    "new_padd = keras.preprocessing.sequence.pad_sequences(new_sequence,maxlen=max_lenght,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 105ms/step\n",
      "[[0.01345476]\n",
      " [0.03960415]\n",
      " [0.869339  ]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(new_padd))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
